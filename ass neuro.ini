import numpy as np

# 1. البيانات:
text = "I love deep learning"
words = text.split()

# نجهز القاموس
word_to_idx = {word: idx for idx, word in enumerate(words)}
idx_to_word = {idx: word for word, idx in word_to_idx.items()}

# نجهز الدخل والهدف
X = [word_to_idx[w] for w in words[:-1]]  # أول 3 كلمات
Y = word_to_idx[words[-1]]  # الكلمة الرابعة

# نعملهم One-hot
vocab_size = len(words)
def one_hot(idx, vocab_size):
    vec = np.zeros(vocab_size)
    vec[idx] = 1
    return vec

X_oh = np.array([one_hot(idx, vocab_size) for idx in X])  # (3, vocab_size)

# 2. تهيئة أوزان الشبكة:
hidden_size = 8  # حجم الذاكرة المخفية

Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden
Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden
Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output

bh = np.zeros((hidden_size, 1))  # bias للطبقة المخفية
by = np.zeros((vocab_size, 1))   # bias للطبقة النهائية

# 3. دالة Softmax
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / np.sum(e_x)

# 4. التدريب:
lr = 0.1
epochs = 500

for epoch in range(epochs):
    hs = np.zeros((hidden_size, 1))  # الحالة المبدئية
    loss = 0

    # Forward pass
    for t in range(3):
        x = X_oh[t].reshape(-1, 1)  # اجعل الدخل عمود
        hs = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hs) + bh)  # حساب الحالة المخفية

    y_pred = np.dot(Why, hs) + by  # الإخراج النهائي
    probs = softmax(y_pred)  # تحويله لاحتمالات

    loss = -np.log(probs[Y][0])  # Cross-entropy loss

    # Backward pass (Gradient descent)
    dy = probs
    dy[Y] -= 1  # Softmax derivative

    dWhy = np.dot(dy, hs.T)
    dby = dy

    dh = np.dot(Why.T, dy) * (1 - hs * hs)  # tanh derivative

    dWxh = np.zeros_like(Wxh)
    dWhh = np.zeros_like(Whh)
    dbh = np.zeros_like(bh)

    for t in range(2, -1, -1):
        x = X_oh[t].reshape(-1, 1)
        dWxh += np.dot(dh, x.T)
        dWhh += np.dot(dh, hs.T)
        dbh += dh

    for param, dparam in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby]):
        param -= lr * dparam

    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

print("\n=== Testing ===")
hs = np.zeros((hidden_size, 1))
for t in range(3):
    x = X_oh[t].reshape(-1, 1)
    hs = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hs) + bh)

y_pred = np.dot(Why, hs) + by
probs = softmax(y_pred)
pred_idx = np.argmax(probs)
predicted_word = idx_to_word[pred_idx]

print(f"Predicted 4th word: {predicted_word}")
print(f"Actual 4th word: {words[-1]}")
